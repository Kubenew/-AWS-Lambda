{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaEFOG02h2z2idJL+Nrbi5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kubenew/-AWS-Lambda/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-VDaUwTO0YT"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.26.2\n",
        "\n",
        "import gym import numpy as np import random import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import matplotlib.pyplot as plt from collections import deque\n",
        "\n",
        "class ProcGenGridWorld(gym.Env): def init(self, size=10, obstacle_prob=0.2): self.size = size self.obstacle_prob = obstacle_prob self.action_space = gym.spaces.Discrete(4) self.observation_space = gym.spaces.Box(0, size-1, (4,), dtype=np.float32)\n",
        "\n",
        "def reset(self):\n",
        "    self.grid = np.zeros((self.size, self.size))\n",
        "    for i in range(self.size):\n",
        "        for j in range(self.size):\n",
        "            if random.random() < self.obstacle_prob:\n",
        "                self.grid[i, j] = 1\n",
        "    self.agent_pos = np.array([0, 0])\n",
        "    self.goal_pos = np.array([self.size-1, self.size-1])\n",
        "    self.grid[0,0] = 0\n",
        "    self.grid[self.size-1,self.size-1] = 0\n",
        "    self.steps = 0\n",
        "    return self._obs()\n",
        "\n",
        "def _obs(self):\n",
        "    return np.concatenate([self.agent_pos, self.goal_pos]).astype(np.float32)\n",
        "\n",
        "def step(self, action):\n",
        "    self.steps += 1\n",
        "    move = [(-1,0),(1,0),(0,-1),(0,1)][action]\n",
        "    new_pos = np.clip(self.agent_pos + move, 0, self.size-1)\n",
        "    if self.grid[new_pos[0], new_pos[1]] == 0:\n",
        "        self.agent_pos = new_pos\n",
        "    dist = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
        "    reward = -0.01\n",
        "    done = False\n",
        "    success = False\n",
        "    if dist < 0.5:\n",
        "        reward = 20\n",
        "        done = True\n",
        "        success = True\n",
        "    elif self.steps > 150:\n",
        "        reward = -5\n",
        "        done = True\n",
        "    return self._obs(), reward, done, {'success': success, 'distance': dist}\n",
        "\n",
        "class DQN(nn.Module): def init(self): super().init() self.net = nn.Sequential( nn.Linear(4, 128), nn.ReLU(), nn.Linear(128,\n",
        "\n",
        "\n",
        "Felix Pankratov\n",
        "Jan 30, 2026, 7:02â€¯PM (14 hours ago)\n",
        "to me\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL HRL SYSTEM FOR PROCGEN - CoinRun Environment with RSI\n",
        "# =============================================================================\n",
        "!pip install procgen gym==0.26.2 torch matplotlib numpy\n",
        "\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque, defaultdict\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from procgen import ProcgenEnv  # Note: ProcgenEnv is vectorized, but we'll use single env\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Procgen Environment Wrapper (CoinRun)\n",
        "# =============================================================================\n",
        "class ProcgenCoinRunWrapper(gym.Env):\n",
        "    def __init__(self, difficulty='easy'):\n",
        "        super().__init__()\n",
        "        self.env = gym.make('procgen:procgen-coinrun-v0',\n",
        "                            start_level=0 if difficulty == 'easy' else 100,\n",
        "                            num_levels=200 if difficulty == 'easy' else 0,\n",
        "                            distribution_mode=difficulty,\n",
        "                            use_backgrounds=False,\n",
        "                            render_mode='rgb_array')\n",
        "\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "\n",
        "        self.current_obs = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = 1000\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_obs = self.env.reset()\n",
        "        self.steps = 0\n",
        "        return self.current_obs\n",
        "\n",
        "    def step(self, action):\n",
        "        next_obs, reward, done, info = self.env.step(action)\n",
        "        self.steps += 1\n",
        "\n",
        "        progress_reward = 0.01 * (self.steps / self.max_steps) if 'level_complete' in info else 0\n",
        "        reward += progress_reward\n",
        "        if self.steps >= self.max_steps:\n",
        "            done = True\n",
        "            reward -= 1.0\n",
        "\n",
        "        self.current_obs = next_obs\n",
        "        success = info.get('level_complete', False)\n",
        "\n",
        "        return next_obs, reward, done, {'success': success, 'progress': progress_reward}\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "# =============================================================================\n",
        "# 2. CNN for Low-Level (Image-based Q-Network)\n",
        "# =============================================================================\n",
        "class CNNQNetwork(nn.Module):\n",
        "    def __init__(self, action_size=15):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 7 * 7, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.tensor(x / 255.0).permute(2, 0, 1).unsqueeze(0).float()\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x).squeeze(0) if x.size(0) == 1 else self.fc(x)\n",
        "\n",
        "class SmartLowLevelController:\n",
        "    def __init__(self, action_size=15, lr=0.0001, gamma=0.99, epsilon_start=0.5):\n",
        "        self.q_net = CNNQNetwork(action_size)\n",
        "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.replay_buffer = deque(maxlen=10000)\n",
        "        self.batch_size = 32\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr  # NEW: RSI will adjust this\n",
        "\n",
        "    def get_action(self, state, subgoal, training=True):\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randint(0, self.action_size - 1)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.q_net(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = np.stack(states)\n",
        "        next_states = np.stack(next_states)\n",
        "\n",
        "        q_values = self.q_net(states).gather(1, torch.tensor(actions).unsqueeze(1)).squeeze(1)\n",
        "        next_q = self.q_net(next_states).max(1)[0]\n",
        "        targets = torch.tensor(rewards) + self.gamma * next_q * (1 - torch.tensor(dones).float())\n",
        "\n",
        "        loss = F.mse_loss(q_values, targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    # NEW: RSI method to adjust hyperparameters\n",
        "    def self_improve_params(self, performance):\n",
        "        if performance < 0.5:  # Low success -> more exploration, faster learning\n",
        "            self.epsilon_decay = max(0.99, self.epsilon_decay - 0.005)  # Slower decay\n",
        "            self.lr *= 1.2  # Increase LR\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = self.lr\n",
        "            print(f\"RSI: Adjusted epsilon_decay to {self.epsilon_decay}, lr to {self.lr}\")\n",
        "        else:\n",
        "            self.lr *= 0.95  # Decrease LR for stability\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = self.lr\n",
        "            print(f\"RSI: Stabilized lr to {self.lr}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Adaptive High-Level with RSI\n",
        "# =============================================================================\n",
        "class AdaptiveHighLevelController:\n",
        "    def __init__(self):\n",
        "        self.performance_history = []\n",
        "        self.good_subgoals = deque(maxlen=100)  # NEW: Store successful subgoals for replay\n",
        "\n",
        "    def select_subgoal(self, state, success_rate):\n",
        "        if random.random() < 0.1 and self.good_subgoals:  # NEW: Occasionally replay good subgoals\n",
        "            return random.choice(self.good_subgoals)\n",
        "        if success_rate > 0.8:\n",
        "            fraction = 0.5\n",
        "        else:\n",
        "            fraction = 0.3\n",
        "        return fraction\n",
        "\n",
        "    def record_performance(self, success, steps, subgoals):\n",
        "        self.performance_history.append({'success': success, 'steps': steps})\n",
        "        if success:  # NEW: Save subgoals from successful episodes\n",
        "            self.good_subgoals.extend(subgoals)\n",
        "\n",
        "    def get_success_rate(self, window=50):\n",
        "        if len(self.performance_history) < window:\n",
        "            return 0.0\n",
        "        recent = self.performance_history[-window:]\n",
        "        return np.mean([p['success'] for p in recent])\n",
        "\n",
        "    # NEW: RSI method for meta-optimization\n",
        "    def self_improve(self, replay_buffer):\n",
        "        if len(replay_buffer) < 32:\n",
        "            return\n",
        "        # Simple meta-fine-tune: Replay high-reward transitions with boosted reward\n",
        "        high_reward_batch = sorted(replay_buffer, key=lambda x: x[2], reverse=True)[:32]  # Top 32\n",
        "        states, actions, rewards, next_states, dones = zip(*high_reward_batch)\n",
        "        states = np.stack(states)\n",
        "        next_states = np.stack(next_states)\n",
        "\n",
        "        # Boost rewards for meta-learning\n",
        "        boosted_rewards = [r * 1.5 for r in rewards]\n",
        "\n",
        "        q_values = self.q_net(states).gather(1, torch.tensor(actions).unsqueeze(1)).squeeze(1)  # Assuming access to q_net\n",
        "        next"
      ]
    }
  ]
}